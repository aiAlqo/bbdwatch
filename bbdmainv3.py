# streamlit_app_prototype.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import warnings
import json
import requests
import os
import pickle
from pathlib import Path
import time # Added for ad simulation

warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="Best Before Date Analysis",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Initialize Session State for Feature Unlocking ---
if 'feature_unlocked_status' not in st.session_state:
    st.session_state.feature_unlocked_status = {}
if 'ads_watched_count' not in st.session_state:
    st.session_state.ads_watched_count = 0
# --- End Session State Initialization ---

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #8a90a0;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .stAlert {
        border-radius: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# Constants and configuration
STAGE_COLORS = {
    'Stage 1': '#2e7d32',
    'Stage 2': '#a5d6a7',
    'Stage 3': '#ffb74d',
    'Expired': '#e57373',
}

STAGE_ORDER = ['Stage 1', 'Stage 2', 'Stage 3', 'Expired']

REQUIRED_COLUMNS = ['BBD', 'Qty On Hand', 'UnitCost', 'Stock Code', 'Description', 'Category']
OPTIONAL_COLUMNS = ['Batch', 'Batch No', 'Location', 'Site', 'MFG Date', 'Manufacture Date']

# LLM Configuration
LLM_PROVIDERS = {
    "OpenAI GPT": "openai",
    "Local Ollama": "ollama",
    "Hugging Face": "huggingface",
    "Custom API": "custom"
}

@st.cache_data
def load_and_validate_data(uploaded_file):
    """Load and validate uploaded CSV data"""
    try:
        df = pd.read_csv(uploaded_file)

        # Check for required columns
        missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]
        if missing_cols:
            st.error(f"‚ùå Missing required columns: {', '.join(missing_cols)}")
            st.info("Required columns: BBD, Qty On Hand, UnitCost, Stock Code, Description, Category")
            return None

        return df
    except Exception as e:
        st.error(f"‚ùå Error reading file: {str(e)}")
        return None

@st.cache_data
def process_data(df, wri_threshold_1, wri_threshold_2):
    """Process and calculate all derived fields"""
    try:
        # Data cleaning and type conversion
        df_processed = df.copy()

        # Handle BBD dates with multiple formats
        df_processed['BBD'] = pd.to_datetime(df_processed['BBD'], errors='coerce', dayfirst=True)

        # Clean quantity data
        df_processed['Qty On Hand'] = (
            df_processed['Qty On Hand']
            .astype(str)
            .str.replace(',', '')
            .str.replace(' ', '')
            .astype(float)
        )

        # Clean cost data
        df_processed['UnitCost'] = pd.to_numeric(df_processed['UnitCost'], errors='coerce')

        # Calculate derived fields
        df_processed['Value'] = df_processed['Qty On Hand'] * df_processed['UnitCost']
        today = pd.to_datetime(datetime.today())
        df_processed['DaysToExpiry'] = (df_processed['BBD'] - today).dt.days

        # Calculate stage
        def stage_func(days): # Renamed from 'stage' to avoid conflict
            if pd.isna(days): return "Unknown"
            elif days > wri_threshold_1: return "Stage 1"
            elif days > wri_threshold_2: return "Stage 2"
            elif days >= 0: return "Stage 3"
            else: return "Expired"

        df_processed['Stage'] = df_processed['DaysToExpiry'].apply(stage_func)

        # Calculate WRI (Waste Risk Index)
        def compute_wri(row):
            if pd.isna(row['DaysToExpiry']) or pd.isna(row['Qty On Hand']):
                return 0
            decay_factor = np.exp(-row['DaysToExpiry'] / 90) if row['DaysToExpiry'] >= 0 else 1.5
            return round(min(100, decay_factor * 100), 1)

        df_processed['WRI'] = df_processed.apply(compute_wri, axis=1)

        # Risk categorization
        def risk_category(wri):
            if wri > 75: return "High"
            elif wri > 50: return "Medium"
            else: return "Low"

        df_processed['Risk Level'] = df_processed['WRI'].apply(risk_category)

        return df_processed

    except Exception as e:
        st.error(f"‚ùå Error processing data: {str(e)}")
        return None

def format_currency(value):
    """Format currency values with proper formatting"""
    if pd.isna(value):
        return "N/A"
    return f"K {value:,.0f}"

def format_number(value):
    """Format numbers with proper separators"""
    if pd.isna(value):
        return "N/A"
    return f"{value:,.0f}"

# --- Feature Unlocking Function ---
def display_watch_ad_button(feature_id: str, unlock_message_noun: str, is_sidebar: bool = False):
    """
    Displays a button to "watch an ad" and unlock a feature.
    """
    button_text = f"Watch Ad to Unlock {unlock_message_noun}"
    button_key = f"watch_ad_button_{feature_id}"

    if st.button(button_text, key=button_key):
        spinner_message = f"üé¨ Simulating ad watch for {unlock_message_noun}... Please wait."
        # Determine context for spinner (main area or sidebar)
        context_spinner = st.sidebar if is_sidebar else st

        with context_spinner.spinner(spinner_message):
            time.sleep(3)  # Simulate ad duration

        st.session_state.feature_unlocked_status[feature_id] = True
        st.session_state.ads_watched_count += 1

        st.success(f"‚úÖ {unlock_message_noun} Unlocked! The section will update.")
        time.sleep(1.5)
        st.rerun()

# --- End Feature Unlocking Function ---

def create_kpi_metrics(df):
    """Create enhanced KPI metrics with better formatting"""
    total_value = df['Value'].sum()
    expired_value = df[df['Stage'] == 'Expired']['Value'].sum()
    high_risk_count = df[df['Risk Level'] == 'High'].shape[0]
    avg_days = df['DaysToExpiry'].mean()

    total_skus = len(df)
    expired_skus = len(df[df['Stage'] == 'Expired'])
    critical_skus = len(df[df['DaysToExpiry'].between(0, 30)])

    return {
        'total_value': total_value, 'expired_value': expired_value,
        'high_risk_count': high_risk_count, 'avg_days': avg_days,
        'total_skus': total_skus, 'expired_skus': expired_skus,
        'critical_skus': critical_skus
    }

@st.cache_data
def generate_llm_insights(chart_data, chart_name_for_llm, df_summary_for_llm): # Renamed args
    """Generate intelligent insights using LLM"""
    llm_provider = st.session_state.get('llm_provider', 'openai')
    api_key = st.session_state.get('api_key', '')

    if not api_key:
        return "Please configure your LLM API key in the sidebar to generate AI insights."

    context = {
        "chart_type": chart_name_for_llm,
        "data_summary": df_summary_for_llm,
        "chart_data": chart_data.to_dict('records') if hasattr(chart_data, 'to_dict') else str(chart_data),
    }
    prompt = f"Analyze chart: {context['chart_type']} with data summary: {context['data_summary']} and chart data: {context['chart_data']}. Provide 3-5 specific, actionable business insights as a bulleted list."

    try:
        if llm_provider == 'openai': return call_openai_api(prompt, api_key)
        elif llm_provider == 'ollama': return call_ollama_api(prompt)
        elif llm_provider == 'huggingface': return call_huggingface_api(prompt, api_key)
        elif llm_provider == 'custom': return call_custom_api(prompt, api_key)
        else: return "Unsupported LLM provider selected."
    except Exception as e: return f"Error generating insights: {str(e)}"

def call_openai_api(prompt, api_key):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"model": "gpt-3.5-turbo", "messages": [{"role": "system", "content": "You are a data analyst."}, {"role": "user", "content": prompt}], "max_tokens": 500, "temperature": 0.7}
    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data, timeout=30)
    if response.status_code == 200: return response.json()['choices'][0]['message']['content']
    else: return f"API Error: {response.status_code} - {response.text}"

def call_ollama_api(prompt):
    data = {"model": "llama2", "prompt": prompt, "stream": False}
    response = requests.post("http://localhost:11434/api/generate", json=data, timeout=30)
    if response.status_code == 200: return response.json()['response']
    else: return f"Ollama API Error: {response.status_code}"

def call_huggingface_api(prompt, api_key):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"inputs": prompt, "parameters": {"max_length": 500, "temperature": 0.7}}
    response = requests.post("https://api-inference.huggingface.co/models/gpt2", headers=headers, json=data, timeout=30)
    if response.status_code == 200: return response.json()[0]['generated_text']
    else: return f"Hugging Face API Error: {response.status_code}"

def call_custom_api(prompt, api_key):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"prompt": prompt, "max_tokens": 500}
    response = requests.post("https://your-custom-api.com/generate", headers=headers, json=data, timeout=30)
    if response.status_code == 200: return response.json().get('response', 'No response from custom API')
    else: return f"Custom API Error: {response.status_code}"

def ensure_data_directory():
    data_dir = Path("historical_data"); data_dir.mkdir(exist_ok=True); return data_dir

def save_analysis_data(df_to_save, filename, upload_date): # Renamed arg
    data_dir = ensure_data_directory()
    metadata = {'filename': filename, 'upload_date': upload_date.isoformat(), 'analysis_date': datetime.now().isoformat(),
                'total_items': len(df_to_save), 'total_value': df_to_save['Value'].sum(),
                'expired_count': len(df_to_save[df_to_save['Stage'] == 'Expired']),
                'high_risk_count': len(df_to_save[df_to_save['Risk Level'] == 'High'])}
    data_file = data_dir / f"{upload_date.strftime('%Y%m%d_%H%M%S')}_{filename}.pkl"
    metadata_file = data_dir / f"{upload_date.strftime('%Y%m%d_%H%M%S')}_{filename}_metadata.json"
    with open(data_file, 'wb') as f: pickle.dump(df_to_save, f)
    with open(metadata_file, 'w') as f: json.dump(metadata, f, indent=2)
    return str(data_file)

def load_historical_data():
    data_dir = ensure_data_directory(); historical_data_list = [] # Renamed
    for data_file in data_dir.glob("*.pkl"):
        metadata_file = data_file.parent / f"{data_file.stem}_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f_meta: metadata = json.load(f_meta)
            with open(data_file, 'rb') as f_data: df_hist = pickle.load(f_data) # Renamed
            historical_data_list.append({'data': df_hist, 'metadata': metadata, 'file_path': str(data_file)})
    historical_data_list.sort(key=lambda x: x['metadata']['upload_date'], reverse=True)
    return historical_data_list

# (compare_datasets and calculate_write_off_risk functions remain unchanged for brevity)
def compare_datasets(current_df, historical_df, current_metadata, historical_metadata):
    """Compare current data with historical data"""
    try:
        comparison = {}
        comparison['total_items'] = {
            'current': len(current_df), 'historical': len(historical_df),
            'change': len(current_df) - len(historical_df),
            'change_pct': ((len(current_df) - len(historical_df)) / len(historical_df)) * 100 if len(historical_df) > 0 else 0
        }
        comparison['total_value'] = {
            'current': current_df['Value'].sum(), 'historical': historical_df['Value'].sum(),
            'change': current_df['Value'].sum() - historical_df['Value'].sum(),
            'change_pct': ((current_df['Value'].sum() - historical_df['Value'].sum()) / historical_df['Value'].sum()) * 100 if historical_df['Value'].sum() > 0 else 0
        } # ... (rest of the function)
        return comparison
    except Exception as e: st.error(f"Error comparing datasets: {str(e)}"); return {}
def calculate_write_off_risk(current_df, historical_df):
    """Calculate potential write-off risk based on movement patterns"""
    try:
        write_off_analysis = {} # ... (rest of the function)
        return write_off_analysis
    except Exception as e: st.error(f"Error calculating write-off risk: {str(e)}"); return {}

# Main app
st.markdown('<h1 class="main-header">BBD Watch Dashboard</h1>', unsafe_allow_html=True)

st.sidebar.header("‚öôÔ∏è Dashboard Settings")
wri_threshold_1 = st.sidebar.slider("Low Risk Threshold (Days)", 0, 365, 180)
wri_threshold_2 = st.sidebar.slider("Medium Risk Threshold (Days)", 0, 365, 90)
uploaded_file = st.sidebar.file_uploader("Upload CSV", type=["csv"])

st.sidebar.subheader("ü§ñ AI Insights Configuration")
llm_provider_key = st.sidebar.selectbox("LLM Provider", options=list(LLM_PROVIDERS.keys())) #Renamed
st.session_state['llm_provider'] = LLM_PROVIDERS[llm_provider_key]
api_key_input = st.sidebar.text_input("API Key", type="password") # Renamed
st.session_state['api_key'] = api_key_input
enable_ai_insights = st.sidebar.checkbox("Enable AI Insights", value=False)

st.sidebar.subheader("üìä Historical Data")
enable_data_persistence = st.sidebar.checkbox("Save Analysis Data", value=True)
historical_data_list = load_historical_data() if enable_data_persistence else [] # Renamed
if historical_data_list:
    st.sidebar.info(f"üìà {len(historical_data_list)} historical analysis files found")
    if len(historical_data_list) > 0:
        historical_options = [f"{h['metadata']['upload_date'][:10]} - {h['metadata']['filename']}" for h in historical_data_list]
        selected_historical_key = st.sidebar.selectbox("Compare with:", options=["None"] + historical_options) # Renamed
        if selected_historical_key != "None":
            selected_index = historical_options.index(selected_historical_key)
            st.session_state['selected_historical'] = historical_data_list[selected_index]
        else: st.session_state['selected_historical'] = None

if uploaded_file:
    df_raw = load_and_validate_data(uploaded_file)
    if df_raw is not None:
        run_analysis_button_placeholder = st.empty() # Placeholder for button
        if run_analysis_button_placeholder.button("Run Analysis", type="primary", use_container_width=True, key="run_analysis_main_btn"):
            st.session_state['analysis_run'] = True
            run_analysis_button_placeholder.empty() # Remove button after click

        if st.session_state.get('analysis_run', False):
            with st.spinner("üîÑ Processing data..."):
                df = process_data(df_raw, wri_threshold_1, wri_threshold_2)

            if df is not None:
                st.success(f"‚úÖ Data loaded! {len(df)} items processed.")
                if enable_data_persistence:
                    saved_file = save_analysis_data(df, uploaded_file.name, datetime.now())
                    if saved_file: st.info(f"üíæ Analysis data saved.")

                if st.button("üîÑ Reset Analysis"):
                    st.session_state['analysis_run'] = False
                    st.rerun()

                st.sidebar.subheader("üîç Filters")
                categories_list = ['All'] + sorted(df['Category'].unique().tolist()) # Renamed
                selected_category = st.sidebar.selectbox("Category", categories_list)
                risk_levels_list = ['All'] + sorted(df['Risk Level'].unique().tolist()) # Renamed
                selected_risk = st.sidebar.selectbox("Risk Level", risk_levels_list)
                stages_filter_list = ['All'] + sorted(df['Stage'].unique().tolist()) # Renamed
                selected_stage_filter = st.sidebar.selectbox("Stage", stages_filter_list) # Renamed
                search_term = st.sidebar.text_input("üîç Search Stock Code/Desc").lower()

                filtered_df = df.copy()
                if selected_category != 'All': filtered_df = filtered_df[filtered_df['Category'] == selected_category]
                if selected_risk != 'All': filtered_df = filtered_df[filtered_df['Risk Level'] == selected_risk]
                if selected_stage_filter != 'All': filtered_df = filtered_df[filtered_df['Stage'] == selected_stage_filter]
                if search_term:
                    mask = (filtered_df['Stock Code'].str.lower().str.contains(search_term, na=False) |
                            filtered_df['Description'].str.lower().str.contains(search_term, na=False))
                    filtered_df = filtered_df[mask]
                if len(filtered_df) != len(df): st.sidebar.info(f"Showing {len(filtered_df)} of {len(df)} items")

                metrics = create_kpi_metrics(filtered_df)
                kpi_cols = st.columns(4)
                kpi_cols[0].markdown(f"<div class='metric-card'><h3>üí∞ Total Value</h3><h2>{format_currency(metrics['total_value'])}</h2><p>{metrics['total_skus']} SKUs</p></div>", unsafe_allow_html=True)
                kpi_cols[1].markdown(f"<div class='metric-card'><h3>‚ö†Ô∏è Expired Stock</h3><h2>{format_currency(metrics['expired_value'])}</h2><p>{metrics['expired_skus']} SKUs</p></div>", unsafe_allow_html=True)
                kpi_cols[2].markdown(f"<div class='metric-card'><h3>üî• High Risk SKUs</h3><h2>{metrics['high_risk_count']}</h2><p>{metrics['critical_skus']} critical</p></div>", unsafe_allow_html=True)
                kpi_cols[3].markdown(f"<div class='metric-card'><h3>üìÖ Avg Days to Expiry</h3><h2>{metrics['avg_days']:.0f} days</h2><p>All items</p></div>", unsafe_allow_html=True)

                st.subheader("üìä Interactive Chart Analysis")
                chart_options = {"Stage Distribution": "sd", "Risk Analysis": "ra", "Expiry Timeline": "et", "Value vs Days": "vd", "Category Analysis": "ca", "Cost Impact": "ci", "Trend Forecast": "tf", "Heatmap": "hm"}
                selected_chart_keys = st.multiselect("Select charts:", options=list(chart_options.keys()), default=["Stage Distribution", "Risk Analysis"]) # Renamed

                if selected_chart_keys:
                    stage_summary_df = filtered_df.groupby('Stage')['Value'].sum().reset_index()
                    for stage_name in STAGE_ORDER: # Renamed
                        if stage_name not in stage_summary_df['Stage'].values:
                            stage_summary_df = pd.concat([stage_summary_df, pd.DataFrame({'Stage': [stage_name], 'Value': [0]})], ignore_index=True)
                    stage_summary_df['Stage'] = pd.Categorical(stage_summary_df['Stage'], categories=STAGE_ORDER, ordered=True)
                    stage_summary_df = stage_summary_df.sort_values('Stage')

                    charts_per_row = 2; chart_cols = st.columns(charts_per_row) # Simplified
                    for i, chart_key_name in enumerate(selected_chart_keys): # Renamed
                        with chart_cols[i % charts_per_row]: # Cycle through columns
                            # ... (Individual chart plotting logic - condensed for brevity, assuming it's correct)
                            st.write(f"Chart: {chart_key_name}") # Placeholder for actual chart

                    st.subheader("üìà Chart Insights")
                    # FEATURE ID: ai_insights_full
                    ai_insights_unlocked = st.session_state.feature_unlocked_status.get('ai_insights_full', False)
                    with st.expander("View analysis insights", expanded=ai_insights_unlocked):
                        if enable_ai_insights and st.session_state.get('api_key'):
                            if ai_insights_unlocked:
                                st.info("ü§ñ Generating AI-powered insights...")
                                # ... (LLM insight generation logic as previously, using renamed vars)
                                for chart_key_name_llm in selected_chart_keys: # Renamed
                                    # Simplified data prep for LLM
                                    chart_data_llm = stage_summary_df if chart_options[chart_key_name_llm] == "sd" else pd.DataFrame({'data':['sample']})
                                    summary_llm = {'total_value': metrics['total_value']}
                                    ai_insight = generate_llm_insights(chart_data_llm, chart_key_name_llm, summary_llm)
                                    st.markdown(f"**{chart_key_name_llm}:** {ai_insight}")
                            else:
                                st.info("Basic insights below. Watch ad for full AI analysis!")
                                display_watch_ad_button('ai_insights_full', "Full AI-Powered Analysis")
                                if not stage_summary_df.empty: st.write(f"‚Ä¢ Basic: {stage_summary_df.iloc[0]['Stage']} is prominent.")
                        else: # AI not enabled or no API key
                            if not stage_summary_df.empty: st.write(f"‚Ä¢ Basic: {stage_summary_df.iloc[0]['Stage']} is prominent.")
                            if not enable_ai_insights and not ai_insights_unlocked: st.info("üí° Enable AI Insights & watch ad for more!")
                            elif not ai_insights_unlocked: display_watch_ad_button('ai_insights_full', "Full AI-Powered Analysis")
                else: st.info("Select charts for visualization.")

                # --- EXPIRED STOCK REPORT ---
                st.subheader("üö® Expired Stock Report")
                expired_report_unlocked = st.session_state.feature_unlocked_status.get('expired_stock_report_full', False)
                with st.expander("View expired stock details", expanded=expired_report_unlocked):
                    expired_items_df = filtered_df[filtered_df['Stage'] == 'Expired'] # Renamed
                    if not expired_items_df.empty:
                        st.warning(f"‚ö†Ô∏è {len(expired_items_df)} items expired. Value: {format_currency(expired_items_df['Value'].sum())}")
                        expired_by_cat_df = expired_items_df.groupby('Category').agg({'Stock Code': 'count', 'Value': 'sum'}).reset_index() # Renamed
                        expired_by_cat_df.columns = ['Category', 'SKU Count', 'Total Value']
                        expired_by_cat_df['Total Value'] = expired_by_cat_df['Total Value'].apply(format_currency)

                        st.write("**Expired by Category (Summary):**")
                        st.dataframe(expired_by_cat_df.head(3) if not expired_report_unlocked else expired_by_cat_df, use_container_width=True)

                        if expired_report_unlocked:
                            st.write("**Detailed Expired Items:**")
                            expired_display_df = expired_items_df[['Stock Code', 'Description', 'Category', 'BBD', 'Qty On Hand', 'Value']].copy() # Renamed
                            expired_display_df['Value'] = expired_display_df['Value'].apply(format_currency)
                            st.dataframe(expired_display_df.sort_values('BBD'), use_container_width=True)
                        else:
                            st.caption("Showing summary & top 3 categories.")
                            display_watch_ad_button('expired_stock_report_full', "Full Expired Stock Report & Details")
                    else: st.success("‚úÖ No expired items!")

                # ... (Other reports - Shelf Life, Near Expiry, etc. - will be modified similarly in subsequent steps if needed)
                # For now, focusing on the two requested: AI Insights and Expired Stock Report.

        else: st.info("üìä Click 'Run Analysis' to process data.")
else:
    st.markdown("## Welcome to BBD Watch Dashboard! üì¶") # Simplified welcome
    st.info("Upload your CSV file using the sidebar to get started!")
    st.subheader("üìÑ Sample Data Structure")
    sample_data = pd.DataFrame({'Stock Code': ['SKU001'], 'Description': ['Sample Item 1'], 'Category': ['Cat A'], 'BBD': ['2024-07-15'], 'Qty On Hand': [100], 'UnitCost': [10.5]})
    st.dataframe(sample_data)
